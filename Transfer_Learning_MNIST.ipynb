{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJmJetfEgiuM"
   },
   "source": [
    "## Transfer Learning using MNIST data\n",
    "To illustrate the power and concept of transfer learning, we will train a CNN on just the digits 5,6,7,8,9.  Then we will train just the last layer(s) of the network on the digits 0,1,2,3,4 and see how well the features learned on 5-9 help with classifying 0-4.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "00J423kMgiuQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import datetime\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hw1x-XVygiuS"
   },
   "outputs": [],
   "source": [
    "#used to help some of the timing functions\n",
    "now = datetime.datetime.now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation took: 0:00:00.000077\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# Your operation here, e.g., training a model\n",
    "# model.fit(...)\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "duration = end_time - start_time\n",
    "print(f\"Operation took: {duration}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0niUiqgegiuS"
   },
   "outputs": [],
   "source": [
    "# set some parameters\n",
    "batch_size = 128\n",
    "num_classes = 5\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Hf7EVUSDgiuT"
   },
   "outputs": [],
   "source": [
    "# set some more parameters\n",
    "img_rows, img_cols = 28, 28\n",
    "filters = 32\n",
    "pool_size = 2\n",
    "kernel_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "4BghnNNXgiuT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Zx5WyUUBgiuU"
   },
   "outputs": [],
   "source": [
    "## To simplify things, write a function to include all the training steps\n",
    "## As input, function takes a model, training set, test set, and the number of classes\n",
    "## Inside the model object will be the state about which layers we are freezing and which we are training\n",
    "#Reshape the data\n",
    "#Normalize the data\n",
    "# One hot encode the targert label\n",
    "# Compile the model\n",
    "# Train the model on the training data\n",
    "# Evaluate the model on the testing data\n",
    "\n",
    "def train_model(model, train, test, num_classes):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def train_model(model, train, test, num_classes):\n",
    "    (x_train, y_train), (x_test, y_test) = train, test\n",
    "    \n",
    "    # Reshape the data based on channels first or last\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x_train = x_train.reshape(x_train.shape[0], 1, 28, 28)\n",
    "        x_test = x_test.reshape(x_test.shape[0], 1, 28, 28)\n",
    "        input_shape = (1, 28, 28)\n",
    "    else:\n",
    "        x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "        x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "        input_shape = (28, 28, 1)\n",
    "    \n",
    "    # Normalize the data\n",
    "    x_train = x_train.astype('float32') / 255\n",
    "    x_test = x_test.astype('float32') / 255\n",
    "    \n",
    "    # One hot encode the target labels\n",
    "    y_train = to_categorical(y_train, num_classes)\n",
    "    y_test = to_categorical(y_test, num_classes)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the training data\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=128,\n",
    "              epochs=10,\n",
    "              verbose=1,\n",
    "              validation_data=(x_test, y_test))\n",
    "    \n",
    "    # Evaluate the model on the testing data\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KejjIrDygiuV",
    "outputId": "4f4073d6-f302-4913-ddcc-8cd6658624fc"
   },
   "outputs": [],
   "source": [
    "# Load the Mnist data and split between train and test sets\n",
    "\n",
    "# create two datasets: one with digits below 5 and one with 5 and above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load the MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Split the dataset into two parts: one for digits < 5 and one for digits >= 5\n",
    "x_train_lt_5 = x_train[y_train < 5]\n",
    "y_train_lt_5 = y_train[y_train < 5]\n",
    "x_test_lt_5 = x_test[y_test < 5]\n",
    "y_test_lt_5 = y_test[y_test < 5]\n",
    "\n",
    "x_train_gte_5 = x_train[y_train >= 5]\n",
    "y_train_gte_5 = y_train[y_train >= 5]\n",
    "x_test_gte_5 = x_test[y_test >= 5]\n",
    "y_test_gte_5 = y_test[y_test >= 5]\n",
    "\n",
    "# Normalize the pixel values to be between 0 and 1\n",
    "x_train_lt_5 = x_train_lt_5.astype('float32') / 255\n",
    "x_test_lt_5 = x_test_lt_5.astype('float32') / 255\n",
    "x_train_gte_5 = x_train_gte_5.astype('float32') / 255\n",
    "x_test_gte_5 = x_test_gte_5.astype('float32') / 255\n",
    "\n",
    "# Reshape the data to fit the model input requirements\n",
    "# Assuming TensorFlow as the backend and channels_last data format\n",
    "x_train_lt_5 = x_train_lt_5.reshape((-1, 28, 28, 1))\n",
    "x_test_lt_5 = x_test_lt_5.reshape((-1, 28, 28, 1))\n",
    "x_train_gte_5 = x_train_gte_5.reshape((-1, 28, 28, 1))\n",
    "x_test_gte_5 = x_test_gte_5.reshape((-1, 28, 28, 1))\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train_lt_5 = to_categorical(y_train_lt_5, 5)\n",
    "y_test_lt_5 = to_categorical(y_test_lt_5, 5)\n",
    "y_train_gte_5 = to_categorical(y_train_gte_5 - 5, 5) # Subtract 5 to make the labels start at 0 for the >=5 subset\n",
    "y_test_gte_5 = to_categorical(y_test_gte_5 - 5, 5)\n",
    "\n",
    "# Now you have two datasets:\n",
    "# - Digits < 5: x_train_lt_5, y_train_lt_5, x_test_lt_5, y_test_lt_5\n",
    "# - Digits >= 5: x_train_gte_5, y_train_gte_5, x_test_gte_5, y_test_gte_5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "oe9E01LIgiuW"
   },
   "outputs": [],
   "source": [
    "# Define the \"feature\" layers. Add 2 convolution layer with max pool layer. At the end, add dropout layer with 0.25% probability and end with the flatten layer. These are the early layers that we expect will \"transfer\"\n",
    "# to a new problem.  We will freeze these layers during the fine-tuning process\n",
    "\n",
    "feature_layers = [\n",
    "   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "\n",
    "feature_layers = [\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "    Flatten()\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "ak-TPz0UgiuX"
   },
   "outputs": [],
   "source": [
    "# Define the \"classification\" layers. Add Dense layer with 128 nodes and the output dense layer. These are the later layers that predict the specific classes from the features\n",
    "# learned by the feature layers.  This is the part of the model that needs to be re-trained for a new problem\n",
    "\n",
    "classification_layers = [\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Assuming num_classes is defined based on your specific problem\n",
    "# For MNIST digits below 5 and digits 5 and above, num_classes would be 5\n",
    "num_classes = 5\n",
    "\n",
    "classification_layers = [\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "2Xr54zn0giuX"
   },
   "outputs": [],
   "source": [
    "# Create the model by combining the two sets of layers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Assuming feature_layers and classification_layers are defined as per previous instructions\n",
    "\n",
    "# Combine the feature and classification layers to create the model\n",
    "model = Sequential(feature_layers + classification_layers)\n",
    "\n",
    "# Now, the model is defined with the combined layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 13, 13, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 5, 5, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 5, 5, 64)          0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 1600)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               204928    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 224389 (876.52 KB)\n",
      "Trainable params: 224389 (876.52 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in feature_layers:\n",
    "    layer.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dQkDZX4CgiuY"
   },
   "outputs": [],
   "source": [
    "# Now, let's train our model on the digits 5,6,7,8,9\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "230/230 [==============================] - 2s 7ms/step - loss: 0.4463 - accuracy: 0.8621 - val_loss: 0.1660 - val_accuracy: 0.9508\n",
      "Epoch 2/10\n",
      "230/230 [==============================] - 2s 7ms/step - loss: 0.1911 - accuracy: 0.9389 - val_loss: 0.1141 - val_accuracy: 0.9650\n",
      "Epoch 3/10\n",
      "230/230 [==============================] - 2s 8ms/step - loss: 0.1485 - accuracy: 0.9530 - val_loss: 0.0886 - val_accuracy: 0.9720\n",
      "Epoch 4/10\n",
      "230/230 [==============================] - 2s 7ms/step - loss: 0.1235 - accuracy: 0.9619 - val_loss: 0.0746 - val_accuracy: 0.9792\n",
      "Epoch 5/10\n",
      "230/230 [==============================] - 2s 7ms/step - loss: 0.1070 - accuracy: 0.9659 - val_loss: 0.0649 - val_accuracy: 0.9792\n",
      "Epoch 6/10\n",
      "230/230 [==============================] - 2s 7ms/step - loss: 0.0978 - accuracy: 0.9697 - val_loss: 0.0618 - val_accuracy: 0.9794\n",
      "Epoch 7/10\n",
      "230/230 [==============================] - 2s 7ms/step - loss: 0.0922 - accuracy: 0.9701 - val_loss: 0.0553 - val_accuracy: 0.9825\n",
      "Epoch 8/10\n",
      "230/230 [==============================] - 2s 7ms/step - loss: 0.0835 - accuracy: 0.9740 - val_loss: 0.0475 - val_accuracy: 0.9856\n",
      "Epoch 9/10\n",
      "230/230 [==============================] - 2s 7ms/step - loss: 0.0813 - accuracy: 0.9743 - val_loss: 0.0476 - val_accuracy: 0.9860\n",
      "Epoch 10/10\n",
      "230/230 [==============================] - 2s 7ms/step - loss: 0.0757 - accuracy: 0.9753 - val_loss: 0.0476 - val_accuracy: 0.9854\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train_gte_5, y_train_gte_5,\n",
    "                    batch_size=128,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test_gte_5, y_test_gte_5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.0475693978369236\n",
      "Test accuracy: 0.9853939414024353\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test_gte_5, y_test_gte_5, verbose=0)\n",
    "print(f'Test loss: {test_loss}')\n",
    "print(f'Test accuracy: {test_accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFQ3FOaggiuZ"
   },
   "source": [
    "### Freezing Layers\n",
    "Keras allows layers to be \"frozen\" during the training process.  That is, some layers would have their weights updated during the training process, while others would not.  This is a core part of transfer learning, the ability to train just the last one or several layers.\n",
    "\n",
    "Note also, that a lot of the training time is spent \"back-propagating\" the gradients back to the first layer.  Therefore, if we only need to compute the gradients back a small number of layers, the training time is much quicker per iteration.  This is in addition to the savings gained by being able to train on a smaller data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "id": "KpNBDuEFgiuZ"
   },
   "outputs": [],
   "source": [
    "# Freeze only the feature layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "\n",
    "# Define feature layers\n",
    "feature_layers = [\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "    Flatten(),\n",
    "]\n",
    "\n",
    "# Define classification layers\n",
    "classification_layers = [\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(5, activation='softmax'),  # Assuming 5 classes for the example\n",
    "]\n",
    "\n",
    "# Combine them into a Sequential model\n",
    "model = Sequential(feature_layers + classification_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in feature_layers:\n",
    "    layer.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Assuming you have your dataset ready\n",
    "# model.fit(x_train, y_train, batch_size=128, epochs=10, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCCEziUTgiua"
   },
   "source": [
    "Observe below the differences between the number of *total params*, *trainable params*, and *non-trainable params*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9lBV_rGbgiua"
   },
   "outputs": [],
   "source": [
    "# print model summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 13, 13, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 5, 5, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 5, 5, 64)          0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 1600)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               204928    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 224389 (876.52 KB)\n",
      "Trainable params: 205573 (803.02 KB)\n",
      "Non-trainable params: 18816 (73.50 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yy-BtW6tgiua"
   },
   "outputs": [],
   "source": [
    "# Now, let's train our model on the digits 0,1,2,3,4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[:-3]:  # Assuming the last 3 layers are your classification layers\n",
    "    layer.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 0.3057 - accuracy: 0.9194 - val_loss: 0.0766 - val_accuracy: 0.9815\n",
      "Epoch 2/10\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 0.1089 - accuracy: 0.9681 - val_loss: 0.0510 - val_accuracy: 0.9858\n",
      "Epoch 3/10\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 0.0814 - accuracy: 0.9758 - val_loss: 0.0370 - val_accuracy: 0.9887\n",
      "Epoch 4/10\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 0.0707 - accuracy: 0.9791 - val_loss: 0.0292 - val_accuracy: 0.9903\n",
      "Epoch 5/10\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 0.0607 - accuracy: 0.9819 - val_loss: 0.0265 - val_accuracy: 0.9907\n",
      "Epoch 6/10\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 0.0530 - accuracy: 0.9840 - val_loss: 0.0240 - val_accuracy: 0.9920\n",
      "Epoch 7/10\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 0.0486 - accuracy: 0.9854 - val_loss: 0.0198 - val_accuracy: 0.9932\n",
      "Epoch 8/10\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 0.0457 - accuracy: 0.9865 - val_loss: 0.0189 - val_accuracy: 0.9934\n",
      "Epoch 9/10\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 0.0436 - accuracy: 0.9865 - val_loss: 0.0165 - val_accuracy: 0.9940\n",
      "Epoch 10/10\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 0.0410 - accuracy: 0.9877 - val_loss: 0.0150 - val_accuracy: 0.9944\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train_lt_5, y_train_lt_5,\n",
    "                    batch_size=128,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test_lt_5, y_test_lt_5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.014963459223508835\n",
      "Test accuracy: 0.9943568706512451\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test_lt_5, y_test_lt_5, verbose=0)\n",
    "print(f'Test loss: {test_loss}')\n",
    "print(f'Test accuracy: {test_accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3E6Z3O-Ugiua"
   },
   "source": [
    "Note that after a single epoch, we are already achieving results on classifying 0-4 that are comparable to those achieved on 5-9 after 5 full epochs.  This despite the fact the we are only \"fine-tuning\" the last layer of the network, and all the early layers have never seen what the digits 0-4 look like.\n",
    "\n",
    "Also, note that even though nearly all (590K/600K) of the *parameters* were trainable, the training time per epoch was still much reduced.  This is because the unfrozen part of the network was very shallow, making backpropagation faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "uJrV-MDhgiub"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
